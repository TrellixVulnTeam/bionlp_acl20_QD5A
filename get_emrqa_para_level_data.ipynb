{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import notebook\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy.random as random\n",
    "from ipdb import set_trace as breakpoint\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_name):\n",
    "    with open(file_name) as f:\n",
    "        dataset_json = json.load(f)\n",
    "    return dataset_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_json = read_json('medication-qa.json')\n",
    "relations_json = read_json('relations-qa.json')\n",
    "risk_json = read_json('risk-qa.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_csv = pd.read_csv('risk-ql.csv', sep=\"\\t\")\n",
    "relations_csv = pd.read_csv('relations-ql.csv', sep='\\t')\n",
    "medication_csv = pd.read_csv('medication-ql.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Was the patient ever prescribed aspirin',\n",
       " 'Has this patient ever been on aspirin',\n",
       " 'Is the patient currently or have they ever taken aspirin',\n",
       " 'has there been a prior aspirin',\n",
       " 'Has this patient ever tried aspirin',\n",
       " 'Has patient ever been prescribed aspirin',\n",
       " 'Has this patient ever been prescribed aspirin',\n",
       " 'Has the patient had multiple aspirin prescriptions',\n",
       " 'has the patient had aspirin',\n",
       " 'Has the patient ever tried aspirin',\n",
       " 'Is there a mention of of aspirin usage/prescription in the record',\n",
       " 'Is there history of use of aspirin',\n",
       " 'Has the pt. ever been on aspirin before',\n",
       " 'aspirin']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medication_json['paragraphs'][0]['qas'][0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30278"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([x['qas'].__len__() for x in medication_json['paragraphs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_logical_form_mapping(curr_csv):\n",
    "    dict_mapping = {}\n",
    "    indices = curr_csv.index\n",
    "    for each_ind in indices:\n",
    "        if(each_ind[0] not in dict_mapping):\n",
    "            dict_mapping[each_ind[0]] = each_ind[1]\n",
    "    return dict_mapping\n",
    "\n",
    "def get_start_end_token(evidence, answer):\n",
    "    start_char_ind = evidence.lower().find(answer.lower())\n",
    "    end_char_ind = start_char_ind+len(answer)\n",
    "    return [evidence, answer, start_char_ind, end_char_ind]\n",
    "\n",
    "def populate_for_all_questions(dataset, curr_ans_list, list_ques, main_logical_form, note_id, data_json_mapping):\n",
    "    return [[dataset, note_id, main_logical_form]+[data_json_mapping[x[0]], x[1], x[0]]+curr_ans_list for x in list_ques]\n",
    "\n",
    "def get_context(sentence, context, PARA_size=10):\n",
    "    orig_sentence = sentence\n",
    "    sentence = sentence.lower()\n",
    "#     sentence = sentence.replace('\\n', '')\n",
    "    orig_context= context\n",
    "    context = [' '.join(x.split()) for x in context]\n",
    "    context = [x.strip().lower() for x in context]\n",
    "    medical_note = ' '.join(context)\n",
    "    medical_note = ' '.join(medical_note.split())\n",
    "    if sentence not in medical_note:\n",
    "        test_dp['note'] = medical_note\n",
    "        test_dp['orig_note'] = orig_context\n",
    "        test_dp['search_sentence'] = sentence\n",
    "        print(valhala)\n",
    "    \n",
    "    start_sentence_idx = medical_note.find(sentence)\n",
    "    end_sentence_idx = start_sentence_idx + len(sentence)\n",
    "    before_part_note = medical_note[:start_sentence_idx]\n",
    "    later_part_note = medical_note[end_sentence_idx:]\n",
    "    before_sents_cnt = random.randint(0,PARA_size)\n",
    "    \n",
    "    before_sents = sent_tokenize(before_part_note)\n",
    "    later_sents = sent_tokenize(later_part_note)\n",
    "    \n",
    "    if len(before_sents)<before_sents_cnt:\n",
    "        before_sents_cnt = len(before_sents)\n",
    "    \n",
    "    rem_sents_cnt = PARA_size - before_sents_cnt\n",
    "    final_para = before_sents[-before_sents_cnt:] + [sentence] + later_sents[:rem_sents_cnt]\n",
    "    final_para = ' '.join(final_para)\n",
    "    final_para = ' '.join(final_para.split())\n",
    "    \n",
    "    return final_para\n",
    "    \n",
    "    flag_not_found = False\n",
    "    for ind, val in enumerate(context):\n",
    "        if(sentence in val):\n",
    "            flag_not_found = True\n",
    "            break\n",
    "#     if flag_not_found:\n",
    "    if True:\n",
    "        chosen_sent_ind = ind\n",
    "        before_sents = random.randint(0,PARA_size)\n",
    "        start_ind = chosen_sent_ind-before_sents\n",
    "        if(start_ind<0):\n",
    "            start_ind = 0\n",
    "        rem = PARA_size - before_sents\n",
    "        end_ind = chosen_sent_ind+rem+1\n",
    "\n",
    "        para = context[start_ind:end_ind]\n",
    "        para = \" \".join(para)\n",
    "    else:\n",
    "        test_dp['sentence'] = sentence\n",
    "        test_dp['context'] = context\n",
    "        test_dp['orig_sentence'] = orig_sentence\n",
    "        print(valhala)\n",
    "    return para\n",
    "    \n",
    "def get_qas_list(data_json, data_json_mapping, dataset=None, PARA_size=10):\n",
    "    all_data_list = []\n",
    "    cnt_yn = 0\n",
    "    cnt_ma = 0\n",
    "    total_size = 0\n",
    "    cnt_not_found = 0\n",
    "    cnt_answer_evidence_sentence_mismatch = 0\n",
    "    rejected_list_yes_no = []\n",
    "    rejected_list_multiple_answers = []\n",
    "    for each_para in notebook.tqdm(data_json['paragraphs']):\n",
    "        note_id = each_para['note_id']\n",
    "        cnt_w = -1\n",
    "        for each_qas in each_para['qas']:\n",
    "            cnt_w+=1\n",
    "            list_ques = each_qas['id'][0]\n",
    "            main_logical_form = each_qas['id'][1]\n",
    "            for each_ans in each_qas['answers']:\n",
    "                total_size+=1\n",
    "                answer_phrase = each_ans['text']\n",
    "                answer_evidence = each_ans['evidence']\n",
    "                if(answer_phrase!=\"\"):\n",
    "                    if(str(type(answer_evidence))==\"<class 'str'>\"):\n",
    "                        answer_phrase = ' '.join(answer_phrase.split())\n",
    "                        answer_evidence = ' '.join(answer_evidence.split())\n",
    "                        answer_found = True if answer_phrase.lower() in answer_evidence.lower() else False\n",
    "                        test_dp['answer_phrase'] = each_ans['text']\n",
    "                        test_dp['answer_evidence'] = each_ans['evidence']\n",
    "                        test_dp['context'] = each_para['context']\n",
    "                        # This condition is when something is wrong and the answer is not matching in the original evidence sentence.\n",
    "                        if not answer_found:\n",
    "                            cnt_answer_evidence_sentence_mismatch+=1\n",
    "                            continue\n",
    "                        answer_evidence = get_context(answer_evidence, each_para['context'], PARA_size)\n",
    "                        curr_ans_list = get_start_end_token(answer_evidence, answer_phrase)\n",
    "                        if(curr_ans_list[2]!=-1):\n",
    "                            curr_ques_ans_list = populate_for_all_questions(dataset,\n",
    "                                                                            curr_ans_list, \n",
    "                                                                            list_ques, \n",
    "                                                                            main_logical_form,\n",
    "                                                                            note_id,\n",
    "                                                                            data_json_mapping)\n",
    "                            all_data_list+=curr_ques_ans_list\n",
    "                        else:\n",
    "                            test_dp['answer_evidence'] = answer_evidence\n",
    "                            test_dp['answer_phrase'] = answer_phrase\n",
    "                            cnt_not_found+=1\n",
    "                    else:\n",
    "                        cnt_ma+=1\n",
    "                        rejected_list_multiple_answers.append(each_ans)\n",
    "                else:\n",
    "                    rejected_list_yes_no.append(each_ans)\n",
    "                    cnt_yn+=1\n",
    "    print('Dataset: ', dataset)                \n",
    "    print('Percentage Rejected Yes No: ', cnt_yn*100.0/total_size)\n",
    "    print('Percentage Rejected MA: ', cnt_ma*100.0/total_size)\n",
    "    print('Percentage Not Found: ', cnt_not_found*100.0/total_size)\n",
    "    print('Percentage evidence phrase sentence mismatch: ', cnt_answer_evidence_sentence_mismatch*100.0/total_size)\n",
    "    print('Total size: ', total_size)\n",
    "    print('Extracted size: ', len(all_data_list))\n",
    "    print('--'*20)\n",
    "    return all_data_list, (rejected_list_yes_no, rejected_list_multiple_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_ques_mapping = get_question_logical_form_mapping(medication_csv)\n",
    "risk_ques_mapping = get_question_logical_form_mapping(risk_csv)\n",
    "relations_ques_mapping = get_question_logical_form_mapping(relations_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072f456f36f8468bae8ab70f9afdc473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  risk\n",
      "Percentage Rejected Yes No:  0.0\n",
      "Percentage Rejected MA:  0.0\n",
      "Percentage Not Found:  0.0\n",
      "Percentage evidence phrase sentence mismatch:  0.8152514737238179\n",
      "Total size:  7973\n",
      "Extracted size:  129991\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "risk_list, rej_risk = get_qas_list(risk_json, risk_ques_mapping, 'risk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb989a5b942e48ebbbc8a2db602cf572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  medication\n",
      "Percentage Rejected Yes No:  40.967512449608726\n",
      "Percentage Rejected MA:  3.967275314204411\n",
      "Percentage Not Found:  0.0\n",
      "Percentage evidence phrase sentence mismatch:  0.4647853924590941\n",
      "Total size:  42170\n",
      "Extracted size:  152905\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "medication_list, rej_med = get_qas_list(medication_json, medication_ques_mapping, 'medication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b53d5b333954372be0691c46609f28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  relations\n",
      "Percentage Rejected Yes No:  74.7969567156313\n",
      "Percentage Rejected MA:  0.0\n",
      "Percentage Not Found:  0.0\n",
      "Percentage evidence phrase sentence mismatch:  0.0\n",
      "Total size:  73999\n",
      "Extracted size:  112153\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "relations_list, rej_rel = get_qas_list(relations_json, relations_ques_mapping, 'relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "152905\n",
      "129991\n",
      "112153\n",
      "------------------------------------------------------------\n",
      "Counter({'empty': 17276})\n",
      "Counter()\n",
      "Counter({'empty': 55349})\n",
      "------------------------------------------------------------\n",
      "Counter({'complex': 1673})\n",
      "Counter()\n",
      "Counter()\n"
     ]
    }
   ],
   "source": [
    "print('--'*30)\n",
    "print(len(medication_list))\n",
    "print(len(risk_list))\n",
    "print(len(relations_list))\n",
    "print('--'*30)\n",
    "print(Counter([x['answer_entity_type'] for x in rej_med[0]]))\n",
    "print(Counter([x['answer_entity_type'] for x in rej_risk[0]]))\n",
    "print(Counter([x['answer_entity_type'] for x in rej_rel[0]]))\n",
    "print('--'*30)\n",
    "print(Counter([x['answer_entity_type'] for x in rej_med[1]]))\n",
    "print(Counter([x['answer_entity_type'] for x in rej_risk[1]]))\n",
    "print(Counter([x['answer_entity_type'] for x in rej_rel[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_dict(list_):\n",
    "    final_list = []\n",
    "    for each_ in list_:\n",
    "        dict_ = {}\n",
    "        dict_['dataset'] = each_[0]\n",
    "        dict_['note_id'] = each_[1]\n",
    "        dict_['logical_form_template'] = each_[2]\n",
    "        dict_['logical_form'] = each_[3]\n",
    "        dict_['question_template'] = each_[4]\n",
    "        dict_['question'] = each_[5]\n",
    "        dict_['evidence_sentence'] = each_[6]\n",
    "        dict_['answer'] = each_[7]\n",
    "        dict_['answer_start_char_ind'] = each_[8]\n",
    "        dict_['answer_end_char_ind'] = each_[9]\n",
    "        final_list.append(dict_)\n",
    "    shuffle(final_list)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "medication_list = convert_list_to_dict(medication_list)\n",
    "risk_list = convert_list_to_dict(risk_list)\n",
    "relations_list = convert_list_to_dict(relations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '100 mg',\n",
       " 'answer_end_char_ind': 1330,\n",
       " 'answer_start_char_ind': 1324,\n",
       " 'dataset': 'medication',\n",
       " 'evidence_sentence': \"he was started back on his oral medication of glipizide 5 mg and was covered with a novolog sliding scale. the patient was transfused 3 units of packed red blood cells postoperatively , and was re-started on coumadin for his peripheral vascular disease. the patient's platelet count dropped to as low as 59 , 000. he had a hit panel sent off which came back negative and platelets trended up at time of discharge to 143 , 000. mr. gier was transferred to the step-down unit on postoperative day #3. his pacing wires were removed , and he was screened for rehabilitation for discharge. the patient also had some urinary retention postoperatively and did require foley reinsertion and was started on flomax 0.4 mg once a day. he failed a second voiding trial and will be discharged with a leg bag and follow up in the urology clinic in one week and continue on his flomax until that time. discharge labs: the discharge labs for mr. howlingwolf were as follows: glucose of 150 , bun of 39 , creatinine 1.2 , sodium 138 , potassium 3.6 , chloride 103 , co2 23 , magnesium 1.9 , wbc 8.6 , hemoglobin 11 , hematocrit 31.4 , platelets of 143 , pt of 17.1 , pt/inr of 1.4. urinalysis from 11/21/06 was negative. urine culture is no growth so far at time of discharge. discharge medications: enteric-coated aspirin 81 mg qd , colace 100 mg b.i.d. while taking dilaudid , lasix 40 mg qd x3 doses , glipizide 5 mg daily , dilaudid 2-4 mg every three hours p.r.n. pain , lisinopril 2.5 mg daily , niferex 150 mg b.i.d.\",\n",
       " 'logical_form': 'MedicationEvent (colace) [dosage=x]',\n",
       " 'logical_form_template': 'MedicationEvent (|medication|) [dosage=x]',\n",
       " 'note_id': '799726',\n",
       " 'question': \"What is the current dose of the patient's colace\",\n",
       " 'question_template': \"What is the current dose of the patient's |medication|\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medication_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logical_form_distr(list_):\n",
    "    all_logical_forms = list(set([x['logical_form_template'] for x in list_]))\n",
    "    print('Total LFs:', len(all_logical_forms))\n",
    "    total_question_templates = 0\n",
    "    train_lf_set = []\n",
    "    dev_lf_set = []\n",
    "    test_lf_set = []\n",
    "    for each_logical_form in all_logical_forms:\n",
    "        print(each_logical_form)\n",
    "        all_question_templates = [x['question_template'] for x in list_ if(x['logical_form_template']==each_logical_form)]\n",
    "        all_question_templates = list(set(all_question_templates))\n",
    "        total_question_templates+=len(all_question_templates)\n",
    "        print(\"No of question templates: \", len(all_question_templates))\n",
    "        if(len(all_question_templates)==1):\n",
    "            train_lf_set+=all_question_templates\n",
    "        elif(len(all_question_templates)==2):\n",
    "            train_lf_set+=all_question_templates[:1]\n",
    "            dev_lf_set+=all_question_templates[1:]\n",
    "            test_lf_set+=all_question_templates[1:]\n",
    "        else:\n",
    "            train_lf_set+=all_question_templates[:int(len(all_question_templates)*0.70)]\n",
    "            dev_lf_set+=all_question_templates[int(len(all_question_templates)*0.70):]\n",
    "            test_lf_set+=all_question_templates[int(len(all_question_templates)*0.70):]\n",
    "        print('--'*20)\n",
    "    print('=='*20)\n",
    "    print('Summary')\n",
    "    print('=='*20)\n",
    "    print(\"Total question templates: \", total_question_templates)\n",
    "    chk = {'train_set':train_lf_set, 'dev_set':dev_lf_set, 'test_set':test_lf_set}\n",
    "    print('Train:', len(chk['train_set']))\n",
    "    print('Development:', len(chk['dev_set']))\n",
    "    print('Test:', len(chk['test_set']))\n",
    "    print('Sanity check, train and dev:', len(chk['train_set'])+len(chk['dev_set']))\n",
    "    print('Sanity check, train and test:', len(chk['train_set'])+len(chk['test_set']))\n",
    "    print('Sanity check, intersection:', set(chk['train_set']).intersection(set(chk['dev_set'])).__len__())\n",
    "    print('Sanity check, intersection:', set(chk['train_set']).intersection(set(chk['test_set'])).__len__())\n",
    "    print('Sanity check, intersection:', set(chk['test_set']).intersection(set(chk['dev_set'])).__len__())\n",
    "    return chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LFs: 30\n",
      "{LabEvent (|test|) [date=x, result=x] OR ProcedureEvent (|test|) [date=x, result=x] OR VitalEvent (|test|) [date=x, result=x]} reveals {ConditionEvent (|problem|) OR SymptomEvent (|problem|)}\n",
      "No of question templates:  2\n",
      "----------------------------------------\n",
      "{MedicationEvent (|treatment|) OR ProcedureEvent (|treatment|)} given {ConditionEvent (x) OR SymptomEvent (x)}\n",
      "No of question templates:  3\n",
      "----------------------------------------\n",
      "{LabEvent (|test|) OR ProcedureEvent (|test|)} conducted {ConditionEvent (x) OR SymptomEvent (x)}\n",
      "No of question templates:  5\n",
      "----------------------------------------\n",
      "{LabEvent (x) OR ProcedureEvent (x) OR VitalEvent (x)} conducted {ConditionEvent (|problem|) OR SymptomEvent (|problem|)}\n",
      "No of question templates:  4\n",
      "----------------------------------------\n",
      "[ProcedureEvent (|treatment|) given/conducted {ConditionEvent (x) OR SymptomEvent (x)}] OR [MedicationEvent (|treatment|) given {ConditionEvent (x) OR SymptomEvent (x)}]\n",
      "No of question templates:  2\n",
      "----------------------------------------\n",
      "LabEvent (x) [date=x, result=x] conducted/reveals ConditionEvent (|problem|)\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "ConditionEvent (|problem|) relates SymptomEvent (x)\n",
      "No of question templates:  6\n",
      "----------------------------------------\n",
      "MedicationEvent (|treatment|) [enddate=x] OR ProcedureEvent (|treatment|) [date=x]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "ConditionEvent (|problem|) [diagnosisdate=x] OR SymptomEvent (|problem|) [onsetdate=x]\n",
      "No of question templates:  12\n",
      "----------------------------------------\n",
      "MedicationEvent (|treatment|) [startdate=x, enddate=x] OR ProcedureEvent (|treatment|) [date=x]\n",
      "No of question templates:  2\n",
      "----------------------------------------\n",
      "{LabEvent (|test|) [date=x, result=x, abnormalResultFlag=Y] OR ProcedureEvent (|test|) [date=x, result=x, abnormalResultFlag=Y] OR [LabEvent (|test|) [date=x, abnormalResultFlag=Y] OR ProcedureEvent (|test|) [date=x, abnormalResultFlag=Y]} reveals {ConditionEvent (x) OR SymptomEvent (x)}]\n",
      "No of question templates:  2\n",
      "----------------------------------------\n",
      "{LabEvent (x) [date=x, result=x] OR ProcedureEvent (x) [date=x, result=x] OR VitalEvent (x) [date=x, result=x]} reveals {ConditionEvent (|problem|) OR SymptomEvent (|problem|)}\n",
      "No of question templates:  5\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [date=x, sortBy(date)] OR ProcedureEvent (|test|) [date=x, sortBy(date)] OR VitalEvent (|test|) [date=x, sortBy(date)]\n",
      "No of question templates:  69\n",
      "----------------------------------------\n",
      "MedicationEvent (|medication|) [sig=x]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "{MedicationEvent (x) OR ProcedureEvent (x)} given {ConditionEvent (|problem|) OR SymptomEvent (|problem|)}\n",
      "No of question templates:  16\n",
      "----------------------------------------\n",
      "MedicationEvent (|medication|) [dosage=x]\n",
      "No of question templates:  7\n",
      "----------------------------------------\n",
      "MedicationEvent (x) given {ConditionEvent (|problem|) OR SymptomEvent (|problem|)}\n",
      "No of question templates:  25\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [(date=x)>(currentDate-|time|), (result=x)>|value|] OR VitalEvent (|test|) [(date=x)>(currentDate-|time|), (result=x)>|value|]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "MedicationEvent (|medication|) [enddate=x]\n",
      "No of question templates:  2\n",
      "----------------------------------------\n",
      "LabEvent (x) [date=x, (result=x)>lab.refhigh]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "ConditionEvent (|problem|) [diagnosisdate=x]\n",
      "No of question templates:  16\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [date=x, (result=x)>lab.refhigh] OR VitalEvent (|test|) [date=x, (result=x)>vital.refhigh]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [date=x, result=x, abnormalResultFlag=Y] OR ProcedureEvent (|test|) [date=x, result=x, abnormalResultFlag=Y]   OR VitalEvent (|test|) [date=x, (result=x)<vital.reflow] OR VitalEvent (|test|) [date=x, (result=x)>vital.refhigh] OR [{LabEvent (|test|) [date=x, abnormalResultFlag=Y] OR ProcedureEvent (|test|) [date=x, abnormalResultFlag=Y] OR VitalEvent (|test|) [date=x]} reveals {ConditionEvent (x) OR SymptomEvent (x)}]\n",
      "No of question templates:  5\n",
      "----------------------------------------\n",
      "ConditionEvent (|problem|) OR SymptomEvent (|problem|)\n",
      "No of question templates:  53\n",
      "----------------------------------------\n",
      "MedicationEvent (|medication|) given {ConditionEvent (x) OR SymptomEvent (x)}\n",
      "No of question templates:  11\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [abnormalResultFlag=Y, date=|date|, result=x] OR ProcedureEvent (|test|) [abnormalResultFlag=Y, date=|date|, result=x] OR VitalEvent (|test|) [date=|date|, (result=x)>vital.refhigh] OR VitalEvent (|test|) [date=|date|, (result=x)<vital.reflow] OR [{LabEvent (|test|) [date=|date|, abnormalResultFlag=Y] OR ProcedureEvent (|test|) [date=|date|, abnormalResultFlag=Y] OR VitalEvent (|test|) [date=|date|]} reveals {ConditionEvent (x) OR SymptomEvent (x)}]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "LabEvent (|test|) [date=x, (result=x)<lab.reflow] OR LabEvent (|test|) [date=x, (result=x)>lab.refhigh] OR VitalEvent (|test|) [date=x, (result=x)<vital.reflow] OR VitalEvent (|test|) [date=x, (result=x)>vital.refhigh]\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "ConditionEvent (|problem|) [status=x] OR SymptomEvent (|problem|) [status=x]\n",
      "No of question templates:  20\n",
      "----------------------------------------\n",
      "{LabEvent (x) [date=x, result=x] OR ProcedureEvent (x) [date=x, result=x] OR VitalEvent (x) [date=x, result=x]} reveals ConditionEvent (|problem|)\n",
      "No of question templates:  6\n",
      "----------------------------------------\n",
      "MedicationEvent (x)\n",
      "No of question templates:  1\n",
      "----------------------------------------\n",
      "========================================\n",
      "Summary\n",
      "========================================\n",
      "Total question templates:  282\n",
      "Train: 192\n",
      "Development: 90\n",
      "Test: 90\n",
      "Sanity check, train and dev: 282\n",
      "Sanity check, train and test: 282\n",
      "Sanity check, intersection: 0\n",
      "Sanity check, intersection: 0\n",
      "Sanity check, intersection: 90\n"
     ]
    }
   ],
   "source": [
    "dict_question_template = get_logical_form_distr(medication_list+risk_list+relations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_train_dev_test(list_, dict_):\n",
    "    total_notes = list(set([x['note_id'] for x in list_]))\n",
    "    train_notes = total_notes[:int(0.60*len(total_notes))]\n",
    "    dev_notes = total_notes[int(0.60*len(total_notes)):int(0.80*len(total_notes))]\n",
    "    test_notes = total_notes[int(0.80*len(total_notes)):]\n",
    "    \n",
    "    train_data = [x for x in list_ if((x['note_id'] in train_notes) \n",
    "                                      and (x['question_template'] in dict_['train_set']))]\n",
    "    dev_data = [x for x in list_ if((x['note_id'] in dev_notes) \n",
    "                                      and (x['question_template'] in dict_['dev_set']))]\n",
    "    test_data = [x for x in list_ if((x['note_id'] in test_notes) \n",
    "                                      and (x['question_template'] in dict_['test_set']))]\n",
    "    \n",
    "    train_data_all_temps = [x for x in list_ if(x['note_id'] in train_notes)]\n",
    "    dev_data_all_temps = [x for x in list_ if(x['note_id'] in dev_notes)]\n",
    "    test_data_all_temps = [x for x in list_ if(x['note_id'] in test_notes)]\n",
    "    \n",
    "    print('=='*20)\n",
    "    print('Strict Split')\n",
    "    print('=='*20)\n",
    "    final_data = {}\n",
    "    final_data['strict_split']={}\n",
    "    final_data['strict_split']['train'] = train_data\n",
    "    final_data['strict_split']['dev'] = dev_data\n",
    "    final_data['strict_split']['test'] = test_data\n",
    "    print('Strict split train: ', len(train_data))\n",
    "    print('Strict split dev: ', len(dev_data))\n",
    "    print('Strict split test: ', len(test_data))\n",
    "    print('Total Strict split: ', len(train_data)+len(dev_data)+len(test_data))\n",
    "    \n",
    "    print('=='*20)\n",
    "    print('Normal Split')\n",
    "    print('=='*20)\n",
    "    final_data['split'] = {}\n",
    "    final_data['split']['train'] = train_data_all_temps\n",
    "    final_data['split']['dev'] = dev_data_all_temps\n",
    "    final_data['split']['test'] = test_data_all_temps\n",
    "    print('Split train: ', len(train_data_all_temps))\n",
    "    print('Split dev: ', len(dev_data_all_temps))\n",
    "    print('Split test: ', len(test_data_all_temps))\n",
    "    print('Total split: ', len(train_data_all_temps)+len(dev_data_all_temps)+len(test_data_all_temps))\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Strict Split\n",
      "========================================\n",
      "Strict split train:  158464\n",
      "Strict split dev:  24729\n",
      "Strict split test:  27226\n",
      "Total Strict split:  210419\n",
      "========================================\n",
      "Normal Split\n",
      "========================================\n",
      "Split train:  235315\n",
      "Split dev:  75802\n",
      "Split test:  83932\n",
      "Total split:  395049\n"
     ]
    }
   ],
   "source": [
    "data_split = divide_train_dev_test(medication_list+risk_list+relations_list, dict_question_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_overlaps(dict_):\n",
    "    train_set_lft = [x['logical_form_template'] for x in dict_['train']]\n",
    "    dev_set_lft = [x['logical_form_template'] for x in dict_['dev']]\n",
    "    test_set_lft = [x['logical_form_template'] for x in dict_['test']]\n",
    "    \n",
    "    train_set_qt = [x['question_template'] for x in dict_['train']]\n",
    "    dev_set_qt = [x['question_template'] for x in dict_['dev']]\n",
    "    test_set_qt = [x['question_template'] for x in dict_['test']]\n",
    "    \n",
    "    print('Num of logical forms in Train:', len(set(train_set_lft)))\n",
    "    print('Num of logical forms in Dev:', len(set(dev_set_lft)))\n",
    "    print('Num of logical forms in Test:', len(set(test_set_lft)))\n",
    "    print('--'*20)\n",
    "    print(\"Train-Dev LF-template Overlap\", len(set(train_set_lft).intersection(set(dev_set_lft))))\n",
    "    print(\"Train-Test LF-template Overlap\", len(set(train_set_lft).intersection(set(test_set_lft))))\n",
    "    print(\"Test-Dev LF-template Overlap\", len(set(test_set_lft).intersection(set(dev_set_lft))))\n",
    "    print('--'*20)\n",
    "    print('Num of question templates in Train:', len(set(train_set_qt)))\n",
    "    print('Num of question templates in Dev:', len(set(dev_set_qt)))\n",
    "    print('Num of question templates in Test:', len(set(test_set_qt)))\n",
    "    print('--'*20)\n",
    "    print(\"Train-Dev Ques-template Overlap\", len(set(train_set_qt).intersection(set(dev_set_qt))))\n",
    "    print(\"Train-Test Ques-template Overlap\", len(set(train_set_qt).intersection(set(test_set_qt))))\n",
    "    print(\"Test-Dev Ques-template Overlap\", len(set(test_set_qt).intersection(set(dev_set_qt))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of logical forms in Train: 30\n",
      "Num of logical forms in Dev: 21\n",
      "Num of logical forms in Test: 21\n",
      "----------------------------------------\n",
      "Train-Dev LF-template Overlap 21\n",
      "Train-Test LF-template Overlap 21\n",
      "Test-Dev LF-template Overlap 21\n",
      "----------------------------------------\n",
      "Num of question templates in Train: 192\n",
      "Num of question templates in Dev: 90\n",
      "Num of question templates in Test: 90\n",
      "----------------------------------------\n",
      "Train-Dev Ques-template Overlap 0\n",
      "Train-Test Ques-template Overlap 0\n",
      "Test-Dev Ques-template Overlap 90\n"
     ]
    }
   ],
   "source": [
    "check_overlaps(data_split['strict_split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of logical forms in Train: 30\n",
      "Num of logical forms in Dev: 30\n",
      "Num of logical forms in Test: 29\n",
      "----------------------------------------\n",
      "Train-Dev LF-template Overlap 30\n",
      "Train-Test LF-template Overlap 29\n",
      "Test-Dev LF-template Overlap 29\n",
      "----------------------------------------\n",
      "Num of question templates in Train: 282\n",
      "Num of question templates in Dev: 282\n",
      "Num of question templates in Test: 281\n",
      "----------------------------------------\n",
      "Train-Dev Ques-template Overlap 282\n",
      "Train-Test Ques-template Overlap 281\n",
      "Test-Dev Ques-template Overlap 281\n"
     ]
    }
   ],
   "source": [
    "check_overlaps(data_split['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_split, open('../data/emrqa_parawise_data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': '100 mg',\n",
       " 'answer_end_char_ind': 1330,\n",
       " 'answer_start_char_ind': 1324,\n",
       " 'dataset': 'medication',\n",
       " 'evidence_sentence': \"he was started back on his oral medication of glipizide 5 mg and was covered with a novolog sliding scale. the patient was transfused 3 units of packed red blood cells postoperatively , and was re-started on coumadin for his peripheral vascular disease. the patient's platelet count dropped to as low as 59 , 000. he had a hit panel sent off which came back negative and platelets trended up at time of discharge to 143 , 000. mr. gier was transferred to the step-down unit on postoperative day #3. his pacing wires were removed , and he was screened for rehabilitation for discharge. the patient also had some urinary retention postoperatively and did require foley reinsertion and was started on flomax 0.4 mg once a day. he failed a second voiding trial and will be discharged with a leg bag and follow up in the urology clinic in one week and continue on his flomax until that time. discharge labs: the discharge labs for mr. howlingwolf were as follows: glucose of 150 , bun of 39 , creatinine 1.2 , sodium 138 , potassium 3.6 , chloride 103 , co2 23 , magnesium 1.9 , wbc 8.6 , hemoglobin 11 , hematocrit 31.4 , platelets of 143 , pt of 17.1 , pt/inr of 1.4. urinalysis from 11/21/06 was negative. urine culture is no growth so far at time of discharge. discharge medications: enteric-coated aspirin 81 mg qd , colace 100 mg b.i.d. while taking dilaudid , lasix 40 mg qd x3 doses , glipizide 5 mg daily , dilaudid 2-4 mg every three hours p.r.n. pain , lisinopril 2.5 mg daily , niferex 150 mg b.i.d.\",\n",
       " 'logical_form': 'MedicationEvent (colace) [dosage=x]',\n",
       " 'logical_form_template': 'MedicationEvent (|medication|) [dosage=x]',\n",
       " 'note_id': '799726',\n",
       " 'question': \"What is the current dose of the patient's colace\",\n",
       " 'question_template': \"What is the current dose of the patient's |medication|\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_split['strict_split']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
